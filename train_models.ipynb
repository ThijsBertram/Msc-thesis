{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL_ivq0npTWM",
        "colab_type": "code",
        "outputId": "aa7a2d59-0971-42ab-d648-efb9939099f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import keras\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.activations import softmax\n",
        "from keras.applications import VGG19\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.models import save_model, load_model\n",
        "from keras.optimizers import SGD\n",
        "from PIL import Image\n",
        "from random import choices\n",
        "from math import log\n",
        "from math import ceil\n",
        "from math import exp\n",
        "from matplotlib.pyplot import xticks\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhnBnWxXSVz-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spsNfXBvpTzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract files to server\n",
        "with ZipFile('data.zip', 'r') as dat:\n",
        "    dat.extractall()\n",
        "\n",
        "# Open up dictionaries that contain info on paritions and labels\n",
        "with open('./partition', 'rb') as pickle_in:\n",
        "    partition = pickle.load(pickle_in)\n",
        "\n",
        "with open('./labels', 'rb') as pickle_in:\n",
        "    labels = pickle.load(pickle_in)\n",
        "\n",
        "# Define global variables\n",
        "img_size = 256\n",
        "crop_size = 224\n",
        "n_classes = 34\n",
        "n_channels = 3\n",
        "batch_size = 136\n",
        "n_epochs = 100    \n",
        "train_dir = './data/'\n",
        "data_dir = './data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RpllCLfpWSF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Functions\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------------\n",
        "# BOOSTING FUNCTIONS\n",
        "# -----------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_beta(i, Dt_dict, error_array, train_IDs, ensemble_method, n_classes=34, prob_array=None):\n",
        "    if ensemble_method == 'M1':\n",
        "        # compute error: error is the sum of distribution probabilities for misclassified samples.\n",
        "        error = np.sum(np.asarray([Dt_dict['{}'.format(i)][train_IDs[i]] if el == 1 else 0 for j, el in enumerate(error_array)]))\n",
        "        # abandon if error is bigger than 0.5\n",
        "        if error > 0.5:\n",
        "            print('error {} > 0.5 at iteration {}, ABORTING'.format(error, i))\n",
        "            return\n",
        "        # set Beta to be error / 1 - error\n",
        "        return error / (1 - error)\n",
        "\n",
        "    elif ensemble_method == 'SAMME':\n",
        "        error_list = []\n",
        "        for j, el in enumerate(error_array):\n",
        "            if el == 1:\n",
        "                error_list.append(Dt_dict['{}'.format(i)][train_IDs[j]])\n",
        "        error_w = sum(error_list)\n",
        "        total_w = sum([el for el in list(Dt_dict['{}'.format(i)].values())])\n",
        "        error = error_w / total_w\n",
        "        return log((1 - error) / error) + log(n_classes - 1)\n",
        "\n",
        "    elif ensemble_method == 'CONFADABOOST':\n",
        "        error = np.sum(np.asarray([(Dt_dict['{}'.format(i)][train_IDs[j]] * max(prob_array[j])) if el == 1 else 0 for j, el in enumerate(error_array)]))\n",
        "        if error > 0.5:\n",
        "            print('error {} > 0.5 at iteration {}, ABORTING'.format(error, i))\n",
        "            return\n",
        "        return 0.5 * log((1 - error) / error)\n",
        "\n",
        "\n",
        "\n",
        "def update_Dt(i, Dt_dict, Beta, error_array, method, train_IDs, prob_array=None):\n",
        "    new_Dt = {}\n",
        "    if method == 'M1':\n",
        "        for j, el in enumerate(error_array):\n",
        "            old_weight = Dt_dict['{}'.format(i)][train_IDs[j]]\n",
        "            # if this sample is misclassified\n",
        "            if el == 1:\n",
        "                new_Dt[train_IDs[j]] = old_weight\n",
        "            else:\n",
        "                product = Beta\n",
        "                new_weight = (old_weight) * product\n",
        "                # add new weight to the final Dt+1 weight list\n",
        "                new_Dt[train_IDs[j]] = new_weight\n",
        "\n",
        "    elif method == 'SAMME':\n",
        "        for j, el in enumerate(error_array):\n",
        "            old_weight = Dt_dict['{}'.format(i)][train_IDs[j]]\n",
        "            if el == 1:\n",
        "                new_Dt[train_IDs[j]] = old_weight\n",
        "            else:\n",
        "                new_weight = old_weight * exp(Beta)\n",
        "                new_Dt[train_IDs[j]] = new_weight\n",
        "\n",
        "    elif method == 'CONFADABOOST':\n",
        "        for j, el in enumerate(error_array):\n",
        "            old_weight = Dt_dict['{}'.format(i)][train_IDs[j]]\n",
        "            if el == 1:\n",
        "                new_Dt[train_IDs[j]] = old_weight\n",
        "            else:\n",
        "                new_weight = old_weight * exp(0.5 - Beta * max(prob_array[j]))\n",
        "                new_Dt[train_IDs[j]] = new_weight\n",
        "\n",
        "    # normalize new weight dict\n",
        "    Zt = np.sum(list(new_Dt.values()))\n",
        "    new_Dt_normalized = dict()\n",
        "    for item in new_Dt.items():\n",
        "        new_Dt_normalized[item[0]] = item[1] / Zt\n",
        "    # new_Dt /= Zt\n",
        "    return new_Dt\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------------\n",
        "# MODEL INITIALIZATION\n",
        "# -----------------------------------------------------------------------------------------------------------------------------\n",
        "def initialize_model(crop_size, n_classes, n_freezed_layers, model='VGG19'):\n",
        "\n",
        "    base_model = VGG19(weights='imagenet', include_top=False, input_tensor=None,\n",
        "                             input_shape=(crop_size, crop_size, 3))\n",
        "\n",
        "    # add a global spatial average pooling layer\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # add a fully-connected layer\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    # added a dropout layer to reduce overfitting\n",
        "    x = Dropout(0.75)(x)\n",
        "    # add a softmax layer\n",
        "    predictions = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    # this is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)  \n",
        "    model.load_weights('top_weights.h5')\n",
        "\n",
        "    for layer in model.layers[:n_freezed_layers]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "#     model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['acc'])\n",
        "    model.compile(optimizer=SGD(lr=0.005, momentum=0.9), loss='categorical_crossentropy', metrics=['acc'])\n",
        "#     model.compile(optimizer=Nadam(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------------\n",
        "# DATAGENERATOR CLASS\n",
        "# -----------------------------------------------------------------------------------------------------------------------------\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "\n",
        "    def __init__(self, list_IDs, labels, data_dir, Dt_dict, i, ensemble_method,\n",
        "                 batch_size=136, n_channels=3, n_classes=34,\n",
        "                 shuffle=True, train=True, DisturbLabel=(False, None),\n",
        "                 crop_size=224, img_size=256):\n",
        "        'Initialization'\n",
        "        self.dim = (crop_size, crop_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.train = train\n",
        "        self.DisturbLabel = DisturbLabel[0]\n",
        "        self.alpha = DisturbLabel[1]\n",
        "        self.possible_labels = list(set(labels.values()))\n",
        "        self.crop_size = crop_size\n",
        "        self.img_size = img_size\n",
        "        self.data_dir = data_dir\n",
        "        self.Dt_dict = Dt_dict\n",
        "        self.i = i\n",
        "        self.ensemble_method = ensemble_method\n",
        "        self.count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        if self.ensemble_method != 'Bagging' and self.train:\n",
        "            X, y, sample_weight = self.__data_generation(list_IDs_temp)\n",
        "            return X, y, sample_weight\n",
        "        else:\n",
        "            X, y = self.__data_generation(list_IDs_temp)\n",
        "            return X, y\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, self.crop_size, self.crop_size, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "        sample_weight = np.empty((self.batch_size), dtype=float)\n",
        "        # Generate data\n",
        "\n",
        "        for n, ID in enumerate(list_IDs_temp):\n",
        "            # Load sample\n",
        "            img = Image.open('/content/data2/{}.jpg'.format(ID))\n",
        "            sample = np.asarray(img.convert('RGB'), dtype=int)\n",
        "#             sample = np.load(self.data_dir + str(ID) + '.npy')\n",
        "\n",
        "            # RANDOM CROP\n",
        "            dx = self.crop_size\n",
        "            dy = self.crop_size\n",
        "            x_pos = np.random.randint(0, self.img_size - dx + 1)\n",
        "            y_pos = np.random.randint(0, self.img_size - dy + 1)\n",
        "            sample = sample[y_pos:(y_pos + dy), x_pos:(x_pos + dx), :]\n",
        "\n",
        "            if self.train:\n",
        "              # HORIZONTAL FLIP with 0.5 chance\n",
        "              if random.randint(0, 1):\n",
        "                  sample = sample[::-1, :, :]\n",
        "                  \n",
        "            # Store sample\n",
        "            X[n,] = sample\n",
        "            # Store class\n",
        "            y[n] = self.labels[ID]\n",
        "            \n",
        "            # GET SAMPLE WEIGHTS FOR BOOSTING\n",
        "            if self.ensemble_method != 'Bagging' and self.train:\n",
        "                # Store sample_weight\n",
        "                if self.count == 0:\n",
        "                  \n",
        "                    print('using sample distribution {}'.format(self.i), self.Dt_dict['{}'.format(self.i)])\n",
        "                    print()\n",
        "                    print()\n",
        "                    self.count += 1\n",
        "\n",
        "                min_sample_weight = min(self.Dt_dict['{}'.format(self.i)].values())\n",
        "                max_sample_weight = max(self.Dt_dict['{}'.format(self.i)].values())\n",
        "                # sample_weight[n] = self.Dt_dict['{}'.format(self.i)][ID] / float(min_sample_weight)\n",
        "                sample_weight[n] = self.Dt_dict['{}'.format(self.i)][ID] \n",
        "            \n",
        "        # DisturbLabel       \n",
        "        if self.DisturbLabel:\n",
        "            disturbed_labels = []\n",
        "            for label in y:\n",
        "                if random.uniform(0, 1) <= self.alpha:\n",
        "                    disturbed_labels.append(random.choice(self.possible_labels))\n",
        "                else:\n",
        "                    disturbed_labels.append(label)\n",
        "            y = disturbed_labels\n",
        "\n",
        "        y = [int(el) for el in y]\n",
        "\n",
        "        if self.ensemble_method != 'Bagging' and self.train:\n",
        "            return preprocess_input(X), keras.utils.to_categorical(y, num_classes=self.n_classes), (sample_weight / max_sample_weight)\n",
        "        else:\n",
        "            return preprocess_input(X), keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
        "\n",
        "        \n",
        "# -----------------------------------------------------------------------------------------------------------------------------\n",
        "# FUNCTION FOR TRAINING INDIVIDUAL CLASSIFIERS\n",
        "# -----------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def train(ensemble_method, n_models=10, n_epochs = 50, alpha=0.1):\n",
        "    # Initialize weight dictionary\n",
        "    Dt_dict = dict()\n",
        "    Dt_dict['0'] = dict()\n",
        "    if ensemble_method == 'SAMME' or ensemble_method == 'CONFADABOOST' or ensemble_method == 'M1':\n",
        "        for ID in partition['train']:\n",
        "            Dt_dict['{}'.format(0)][ID] = 1 / len(partition['train'])\n",
        "    else:\n",
        "        for ID in partition['train']:\n",
        "            Dt_dict['{}'.format(0)][ID] = 1\n",
        "            \n",
        "            \n",
        "    with open('Dt_dic-9', 'rb') as pickle_in:\n",
        "      Dt_dict['9'] = pickle.load(pickle_in)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------------------------\n",
        "    # TRAINING\n",
        "    # ------------------------------------------------------------------------------------------------------------------\n",
        "    val_acc_list = []\n",
        "    for i in range(n_models):\n",
        "        # -------------------------------\n",
        "        # Subset if BAGGING\n",
        "        # -------------------------------\n",
        "        if ensemble_method == 'Bagging':\n",
        "              train_IDs = choices(partition['train'], k=len(partition['train']))\n",
        "        else:\n",
        "            train_IDs = partition['train']\n",
        "\n",
        "        # -------------------------------\n",
        "        # DEFINE GENERATORS\n",
        "        # -------------------------------\n",
        "        if ensemble_method == 'Disturblabel':\n",
        "          training_generator = DataGenerator(train_IDs, labels,\n",
        "                                         batch_size=batch_size, ensemble_method=ensemble_method,\n",
        "                                         shuffle=True,\n",
        "                                         n_classes=n_classes,\n",
        "                                         train=True,\n",
        "                                         DisturbLabel=(True, alpha),\n",
        "                                         data_dir=data_dir, Dt_dict=Dt_dict, i=i)\n",
        "        else:\n",
        "          training_generator = DataGenerator(train_IDs, labels,\n",
        "                                             batch_size=batch_size, ensemble_method=ensemble_method,\n",
        "                                             shuffle=True,\n",
        "                                             n_classes=n_classes,\n",
        "                                             train=True,\n",
        "                                             DisturbLabel=(False, 0.1),\n",
        "                                             data_dir=data_dir, Dt_dict=Dt_dict, i=i)\n",
        "        \n",
        "        \n",
        "     \n",
        "\n",
        "        validation_generator = DataGenerator(partition['val'], labels, ensemble_method=ensemble_method,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             n_classes=n_classes,\n",
        "                                             train=False,\n",
        "                                             data_dir=data_dir, Dt_dict=Dt_dict, i=i)\n",
        "\n",
        "        boosting_generator = DataGenerator(train_IDs, labels, ensemble_method=ensemble_method,\n",
        "                                           batch_size=1,\n",
        "                                           shuffle=False,\n",
        "                                           n_classes=n_classes,\n",
        "                                           train=False,\n",
        "                                           data_dir=data_dir, Dt_dict=Dt_dict, i=i)\n",
        "\n",
        "        testing_generator = DataGenerator(partition['test'], labels, ensemble_method=ensemble_method,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=False,\n",
        "                                          n_classes=n_classes,\n",
        "                                          train=False,\n",
        "                                          data_dir=data_dir, Dt_dict=Dt_dict, i=i)\n",
        "\n",
        "        # --------------------------------------------------------------------------------------------------------------\n",
        "        # TRAIN MODEL AND SAVE IT\n",
        "        # --------------------------------------------------------------------------------------------------------------\n",
        "        model = initialize_model(224, 34, 13)\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=10)\n",
        "        hist = model.fit_generator(generator=training_generator,\n",
        "                                   steps_per_epoch=len(train_IDs) / batch_size,\n",
        "                                   validation_data=validation_generator,\n",
        "                                   validation_steps=len(partition['val']) / batch_size,\n",
        "                                   epochs=n_epochs,\n",
        "                                   use_multiprocessing=False,\n",
        "                                   verbose=1,\n",
        "                                   callbacks=[early_stopping])\n",
        "\n",
        "        # save model\n",
        "        if ensemble_method == 'Distrublabel':\n",
        "          model.save('{}-model'.format(ensemble_method))\n",
        "        else:\n",
        "          model.save('{}-model-{}'.format(ensemble_method, i))\n",
        "\n",
        "        # ----------------------------------\n",
        "        # TEST PREDICTIONS\n",
        "        # ----------------------------------\n",
        "        y_test_prob = model.predict_generator(testing_generator, steps=len(partition['test']), verbose=1)\n",
        "        np.save('pred_prob-{}-model-{}'.format(ensemble_method, i), y_test_prob)\n",
        "        # ---------------------------------------------\n",
        "        # BOOSTING ONLY: UPDATE SAMPLE DISTRIBUTION D_t\n",
        "        # ---------------------------------------------\n",
        "        if ensemble_method != 'Bagging' and ensemble_method != 'Disturblabel':\n",
        "            print('updating weights for {} time'.format(i))\n",
        "            # Prediction array of (len(X_train), n_classes) with probabilities as values\n",
        "            y_train_prob = model.predict_generator(boosting_generator, steps=len(train_IDs), verbose=1)\n",
        "            np.save('train-{}-model-{}', y_train_prob)\n",
        "            # Transform prediction probabilities to a list of predicted classes\n",
        "            y_train_pred = [np.argmax(row) for row in y_train_prob]\n",
        "            y_train_true = [int(labels[ID]) for ID in train_IDs]\n",
        "            # Create an erray that represents misclassification errors as Booleans\n",
        "            error_array = np.asarray([0 if pred == y_train_true[i] else 1 for i, pred in enumerate(y_train_pred)])\n",
        "            # Compute new sample distribution from Betas\n",
        "            Beta = compute_beta(i, Dt_dict, error_array=error_array, ensemble_method=ensemble_method, n_classes=34, prob_array=y_train_prob, train_IDs=train_IDs)\n",
        "            new_Dt = update_Dt(i, Dt_dict, Beta, error_array=error_array, method=ensemble_method, prob_array=y_train_prob, train_IDs=train_IDs)\n",
        "            # Add new sample distribution to distribution dictionairy Dt_dict\n",
        "            Dt_dict['{}'.format(i + 1)] = new_Dt\n",
        "            with open('Dt_dic-{}'.format(i+1), 'wb') as pickle_out:\n",
        "              pickle.dump(new_Dt, pickle_out)\n",
        "           \n",
        "    return        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnt5YzZCpWeM",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Top layer training\n",
        "training_generator = DataGenerator(partition['train'], labels,\n",
        "                                           batch_size=batch_size, ensemble_method=ensemble_method,\n",
        "                                           shuffle=True,\n",
        "                                           n_classes=n_classes,\n",
        "                                           train=True,\n",
        "                                           DisturbLabel=(False, 0.1),\n",
        "                                           data_dir=data_dir, Dt_dict=Dt_dict, i=i)\n",
        "\n",
        "validation_generator = DataGenerator(partition['val'], labels, ensemble_method=ensemble_method,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             n_classes=n_classes,\n",
        "                                             train=False,\n",
        "                                             data_dir=data_dir, Dt_dict=Dt_dict, i=i)\n",
        "\n",
        "model = initialize_model(224, 34, 10)\n",
        "    \n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
        "hist = model.fit_generator(generator=training_generator,\n",
        "                                   steps_per_epoch=len(partition['train']) / batch_size,\n",
        "                                   validation_data=validation_generator,\n",
        "                                   validation_steps=len(partition['val']) / batch_size,\n",
        "                                   epochs=20,\n",
        "                                   use_multiprocessing=False,\n",
        "                                   verbose=1)\n",
        "\n",
        "model.save_weights('top_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qECMqsOwpWks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bagging\n",
        "train(ensemble_method='Bagging', n_epochs=100)\n",
        "\n",
        "# Boosting \n",
        "train(ensemble_method='M1', n_epochs=100)\n",
        "train(ensemble_method='Disturblabel', alpha=0.1, n_models = 1, n_epochs=100)\n",
        "train(ensemble_method='Disturblabel', alpha=0.2, n_models = 1, n_epochs=100)\n",
        "train(ensemble_method='CONFADABOOST', n_epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}